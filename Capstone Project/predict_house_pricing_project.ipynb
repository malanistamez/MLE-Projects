{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"c0111f8d98b300cddecdb814fd4fa6548c49a866"},"source":["### **Capstone Project Proposal: Predicting Housing Prices with Machine Learning**\n","\n","\n","## Background\n","I'm excited to delve into the realm of machine learning with my first project using AWS SageMaker. My aim is to explore data analysis and model training on the SageMaker platform by tackling the challenge of predicting housing prices.\n","\n","## Contents\n","\n","1. [Background](#Background)\n","2. [Approach](#Approach)\n","3. [Environment](#Environment)\n","4. [Data](#Data)\n","5. [SageMaker](#SageMaker)\n","6. [Hyperparameters](#Hyperparameters)\n","7. [Conclusions](#Conclusions)\n","\n","## Approach\n","\n","For this project, I'm not aiming for a top-ranking solution in the \"House Prices\" Kaggle competition. Instead, I'm focusing on leveraging SageMaker's infrastructure to develop a model that outperforms a naive solution. Given the wealth of information available on the Kaggle competition page, I'll skip extensive data exploration and feature engineering.\n","\n","My plan is to use a default algorithm provided by SageMaker, with eXtreme Gradient Boosting (XGBoost) being a promising choice for regression problems like this. To kickstart the process, I'll utilize a SageMaker example notebook on XGBoost, allowing me to bypass syntax hunting and focus on the workflow.\n","\n","## Environment\n","\n","### AWS Setup\n","1. Begin by setting up an AWS account on aws.amazon.com.\n","2. Access the SageMaker service and create a new notebook instance.\n","3. Opt for a cost-effective instance type like \"ml.t2.medium\" to keep expenses minimal.\n","4. Utilize the default IAM role for simplicity.\n","5. Start the notebook instance and open a terminal for future use.\n","\n","## Data\n","\n","I'll be using the dataset provided by the \"House Prices\" Kaggle competition. This dataset contains various features such as square footage, number of bedrooms, and location details, which will be used to predict housing prices.\n","\n","## SageMaker\n","\n","I'll employ AWS SageMaker for data preprocessing, model training, and evaluation. This cloud-based platform offers a streamlined workflow for building and deploying machine learning models at scale.\n","\n","## Hyperparameters\n","\n","Given the simplicity of this project, I'll primarily rely on default hyperparameters provided by the chosen algorithm. However, I'll explore tuning them if time permits to enhance model performance.\n","\n","## Conclusions\n","\n","In this project, I aim to gain hands-on experience with AWS SageMaker and machine learning workflows. While the focus is not on achieving top-tier performance in the Kaggle competition, I expect to develop a functional model that demonstrates proficiency in utilizing SageMaker's capabilities. Moving forward, I plan to explore more advanced techniques and delve deeper into feature engineering for improved predictions."]},{"cell_type":"markdown","metadata":{"_uuid":"0564a9e562ab7d98ac645ea90834f84505b013bd"},"source":["### Enabling Kaggle Tools\n","For convenience, let's enable the Kaggle API and command line tools. First, we install the Python package to get the command line tools."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"291102e642dcbb2393aff02d384941b100070e3d"},"outputs":[],"source":["!pip install kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"18de81aaa7b56eaefcdeaa641a6c4e11440c5c49"},"outputs":[],"source":["!kaggle competitions list"]},{"cell_type":"markdown","metadata":{"_uuid":"488115f1b00314f26fa58bcf713ed6dcac314ac7"},"source":["__Kaggle API__\n","\n","Then we enable the Kaggle API. This assumes you have an account on Kaggle. It's free and only takes a minute. Once you have that, follow instructions here to retrieve your kaggle.json file\n","\n","https://github.com/Kaggle/kaggle-api\n","\n","Using the AWS Jupyter files tab, upload your kaggle.json. I had to use the Jupyter terminal to move it to ~/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"be36c83b328401dfa411aafa8fee764915fccbb2"},"outputs":[],"source":["!mkdir ~/.kaggle\n","!mv kaggle.json ../.kaggle/"]},{"cell_type":"markdown","metadata":{"_uuid":"756cc68d55bf43d5afbeca8fe37b72160ef27a8c"},"source":["Finally, we follow the advice to make sure our Kaggle key isn't readable by other users of this system. This is a corner we could have cut on this private EC2 instance."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"50398291fdeb3620b22f68e3d71639a166e01bad"},"outputs":[],"source":["!chmod 600 ../.kaggle/kaggle.json"]},{"cell_type":"markdown","metadata":{"_uuid":"ec662db184758865108721b6281a7cb5f38bba0a"},"source":["### Python and SageMaker Setup\n","\n","Now we start coding in Python. We import the necessary libraries and create a connection to the SageMaker service."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"221ad79507c1349cf16d43a85793c92404c4b798"},"outputs":[],"source":["import numpy as np                                # For matrix operations and numerical processing\n","import pandas as pd                               # For munging tabular data\n","import matplotlib.pyplot as plt                   # For charts and visualizations\n","from IPython.display import Image                 # For displaying images in the notebook\n","from IPython.display import display               # For displaying outputs in the notebook\n","from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n","import sys                                        # For writing outputs to notebook\n","import math                                       # For ceiling function\n","import json                                       # For parsing hosting outputs\n","import os                                         # For manipulating filepath names\n","import sagemaker                                  # Amazon SageMaker's Python SDK provides many helper functions\n","from sagemaker.predictor import csv_serializer    # Converts strings for HTTP POST requests on inference"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c66f958e78220c1fce2886c96af6c05da786b0d7","isConfigCell":true},"outputs":[],"source":["bucket = sagemaker.Session().default_bucket()\n","prefix = 'sagemaker/house_price_xgboost'\n"," \n","# Define IAM role\n","import boto3\n","import re\n","from sagemaker import get_execution_role\n","\n","role = get_execution_role()\n","region = boto3.Session().region_name \n","smclient = boto3.Session().client('sagemaker')"]},{"cell_type":"markdown","metadata":{"_uuid":"6a48cfecc4c4c8bbb20343dd5fc3eeb014d71d4f"},"source":["\n","\n","## Data\n","\n","### Retrieval\n","Let's start by downloading the [direct marketing dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) from UCI's ML Repository."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"602d2b0a8f42f6958e7c2815b6439fcb48b90758"},"outputs":[],"source":["!kaggle competitions list"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f89922ed73966588639f2f1c9d2ee968a9c4e283"},"outputs":[],"source":["!kaggle competitions download -p ./data house-prices-advanced-regression-techniques"]},{"cell_type":"markdown","metadata":{"_uuid":"569213b66bef5eac9e1d766bc3ad28ad07abceb7"},"source":["### Exploration\n","Now lets read this into a Pandas data frame and take a look, but just a quick one. Normally, we would investigate the data. But because our goal is to get a model trained end-to-end on SageMaker as quickly as possible, we'll skip all those best practices and see what happens."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f260d1ecf69a418e35c6f4170356bf7e3f6f75c1"},"outputs":[],"source":["df_train = pd.read_csv('./data/train.csv')\n","pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n","df_train.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"c4d4be6e42d85d395b9ed22f0cf9d633b127bbff"},"source":["### First submission\n","\n","Before we go further investigating the data and creating sophisticated predictive models, let's make sure we're able to submit something, anything, to the competition. The idea is to validate our pipeline and that we've understood the submission format. And by getting an initial score, we're better able to judge future improvements.\n","\n","The required format is provided in the \"sample_submission.csv\" file retrieved earlier."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f713cb7efc16027d0c295af4c8b17fa3862bff36"},"outputs":[],"source":["!head ./data/sample_submission.csv"]},{"cell_type":"markdown","metadata":{"_uuid":"63c8571298fef75dbb4f1181ed6c2216a854ae01"},"source":["For the purpose of test driving the submission process, we don't need a good result, just a valid one. A trivial attempt would be to use the means of the training set as the answer for all items in the testing set."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9c782d5fc3071b23817cd3f404816e511f62adc7"},"outputs":[],"source":["df_train.describe()['SalePrice']"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9f64867527e63d7ce9e881f5ca2a42f6d3657b2d"},"outputs":[],"source":["df_competition = pd.read_csv('./data/test.csv')\n","df_competition.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"68ede9d9848cc524983bf1e3ee598b3a3938a468"},"outputs":[],"source":["df_submit = pd.DataFrame(df_competition['Id'], dtype=int)\n","df_submit['SalePrice'] = df_train.describe()['SalePrice']['mean']\n","df_submit.head()\n","df_submit.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b59268a703d9d9fa561bfa3df38d77148a73692a"},"outputs":[],"source":["df_submit.to_csv('./data/sub_mean.csv',index=False)\n","!head ./data/sub_mean.csv\n","!tail ./data/sub_mean.csv\n"]},{"cell_type":"markdown","metadata":{"_uuid":"adeab0ba28d11737e56f62ee740833a3767b0c46"},"source":["That is the format we were looking for. Now let's try to submit it through the Kaggle API:\n","\n","> usage: kaggle competitions submit [-h] -f FILE_NAME -m MESSAGE [-q] [competition]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ca60a4dc5caed4bd67fddb40206e2053e7191143"},"outputs":[],"source":["!kaggle competitions submit -f ./data/sub_mean.csv -m \"Means-based submission\" house-prices-advanced-regression-techniques\n","#\n","# Need UI buffer space so horizontal scrollbar does not get in the way"]},{"cell_type":"markdown","metadata":{"_uuid":"f1f9130b034a76379608ba1fabe1f1311894f6a8"},"source":["On Kaggle.com under \"My submission\", I can see that this trivial technique gives a public score of 0.42949. At the time of this writing, this technique places us 4515 out of 4745 on the public leaderboard. Clearly not a good score, but we didn't expect it to be.\n","\n","Now that our submission pipeline has been validated, we can turn our attention to submitting something more clever through SageMaker."]},{"cell_type":"markdown","metadata":{"_uuid":"853dd0061e1f45425e37b6d612cb48ec3bc2cb09"},"source":["### Data Transformation\n","\n","To create a more sophisticated solution, we should know our data. The pros urge us to do the following:\n","\n","> Cleaning up data is part of nearly every machine learning project.  It arguably presents the biggest risk if done incorrectly and is one of the more subjective aspects in the process.  Several common techniques include:\n","\n",">* Handling missing values: Some machine learning algorithms are capable of handling missing values, but most would rather not.  Options include:\n"," * Removing observations with missing values: This works well if only a very small fraction of observations have incomplete information.\n"," * Removing features with missing values: This works well if there are a small number of features which have a large number of missing values.\n"," * Imputing missing values: Entire [books](https://www.amazon.com/Flexible-Imputation-Missing-Interdisciplinary-Statistics/dp/1439868247) have been written on this topic, but common choices are replacing the missing value with the mode or mean of that column's non-missing values.\n","* Converting categorical to numeric: The most common method is one hot encoding, which for each feature maps every distinct value of that column to its own feature which takes a value of 1 when the categorical feature is equal to that value, and 0 otherwise.\n","* Oddly distributed data: Although for non-linear models like Gradient Boosted Trees, this has very limited implications, parametric models like regression can produce wildly inaccurate estimates when fed highly skewed data.  In some cases, simply taking the natural log of the features is sufficient to produce more normally distributed data.  In others, bucketing values into discrete ranges is helpful.  These buckets can then be treated as categorical variables and included in the model when one hot encoded.\n","* Handling more complicated data types: Mainpulating images, text, or data at varying grains is left for other notebook templates.\n","\n","> Luckily, some of these aspects have already been handled for us, and the algorithm we are showcasing tends to do well at handling sparse or oddly distributed data.  Therefore, let's keep pre-processing simple.\n","\n","So, for the purpose of racing to the end, we'll disregard most of it and simply convert categorical variables to numerical data"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"40f242973360a35041c57c8ffeb5e45b7c865335"},"outputs":[],"source":["df_train = pd.get_dummies(df_train)   # Convert categorical variables to sets of indicators\n","df_train.describe()"]},{"cell_type":"markdown","metadata":{"_uuid":"0ab66615d72f348165ae1e4d9cd4293e004c5b88"},"source":["### Feature engineering\n","\n","If we cared about the score, we would totally look into that."]},{"cell_type":"markdown","metadata":{"_uuid":"446a8545ba25750cac699efdc6a98e1c7a8c5e2b"},"source":["We, however, will heed this advice regarding creating a validation test from our test data:\n","\n","> When building a model whose primary goal is to predict a target value on new data, it is important to understand overfitting.  Supervised learning models are designed to minimize error between their predictions of the target value and actuals, in the data they are given.  This last part is key, as frequently in their quest for greater accuracy, machine learning models bias themselves toward picking up on minor idiosyncrasies within the data they are shown.  These idiosyncrasies then don't repeat themselves in subsequent data, meaning those predictions can actually be made less accurate, at the expense of more accurate predictions in the training phase.\n","\n","> The most common way of preventing this is to build models with the concept that a model shouldn't only be judged on its fit to the data it was trained on, but also on \"new\" data.  There are several different ways of operationalizing this, holdout validation, cross-validation, leave-one-out validation, etc.  For our purposes, we'll simply randomly split the data into 3 uneven groups.  The model will be trained on 70% of data, it will then be evaluated on 20% of data to give us an estimate of the accuracy we hope to have on \"new\" data, and 10% will be held back as a final testing dataset which will be used later on."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"fdaddc94647214d8a8010196fa8a19e9b1214c80"},"outputs":[],"source":["#model_data = data\n","train_data, validation_data, test_data = np.split(df_train.sample(frac=1, random_state=1729), [int(0.7 * len(df_train)), int(0.9*len(df_train))])  "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7f8e9431bfc4cb46751071672dea48759b3d8675"},"outputs":[],"source":["train_data.shape"]},{"cell_type":"markdown","metadata":{"_uuid":"41d45f009644f45277e1bbae3b7295bc2e6d0d5a"},"source":[">Amazon SageMaker's XGBoost container expects data in the libSVM or CSV data format.  For this example, we'll stick to CSV.  Note that the first column must be the target variable and the CSV should not include headers.  Also, notice that although repetitive it's easiest to do this after the train|validation|test split rather than before.  This avoids any misalignment issues due to random reordering."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b9ae9d642184a0bc13c19b1d32a4ed72c2d74f8c"},"outputs":[],"source":["#pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n","#pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n","pd.concat([train_data['SalePrice'], train_data.drop(['SalePrice'], axis=1)], axis=1).to_csv('./data/sm_train.csv', index=False, header=False)\n","pd.concat([validation_data['SalePrice'], validation_data.drop(['SalePrice'], axis=1)], axis=1).to_csv('./data/sm_validation.csv', index=False, header=False)\n","!ls -l ./data"]},{"cell_type":"markdown","metadata":{"_uuid":"0f245d8b920c5b05846afd883ecefd079e799518"},"source":["Now we'll copy the file to S3 for Amazon SageMaker's managed training to pickup."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ce5182b5753983224fdabbbd828e0182244f1490"},"outputs":[],"source":["boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('./data/sm_train.csv')\n","boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('./data/sm_validation.csv')"]},{"cell_type":"markdown","metadata":{"_uuid":"0f82aba28c17baaa339df7f37aecac9e6564910e"},"source":["\n","## SageMaker Training with XGBoost\n","\n","* Introduction to XGBoost\n","\n","We've chosen XGBoost for our SageMaker training because it's tailor-made for the job. \n","Our dataset presents skewed distributions, correlated features, and non-linear relationships \n","with the target variable. Additionally, we prioritize predictive accuracy over interpretability \n","for future prospect targeting, making gradient boosted trees an ideal choice.\n","\n","\n","* Overview of Gradient Boosted Trees\n","\n","\n","At a fundamental level, gradient boosted trees excel by amalgamating predictions from \n","numerous simple models, each refining the weaknesses of its predecessors. This collaborative \n","approach often outshines the performance of complex models. \n","Further insights into gradient boosting and its distinctions from similar algorithms are \n","available in other Amazon SageMaker resources.\n","\n","\n","* The Power of XGBoost\n","\n","\n","XGBoost, a leading open-source package for gradient boosted trees, stands out for its \n","computational prowess, rich feature set, and impressive track record in machine learning \n","competitions. Let's kickstart our journey with a straightforward XGBoost model, leveraging \n","Amazon SageMaker's managed, distributed training infrastructure.\n","\n","\n","* Specifying the ECR Container Location\n","\n","container = get_image_uri(boto3.Session().region_name, 'xgboost')\n","\n","* Uploading Data to S3\n","\n","s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n","s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')\n","\n","* Configuring SageMaker Estimator \n","\n","xgb_estimator = sagemaker.estimator.Estimator(container,\n","                                    role, \n","                                    train_instance_count=1, \n","                                    train_instance_type='ml.m4.xlarge',\n","                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n","                                    sagemaker_session=sess)\n","\n","* Setting Hyperparameters for XGBoost\n","\n","xgb_estimator.set_hyperparameters(max_depth=5,\n","                        eta=0.2,\n","                        gamma=4,\n","                        min_child_weight=6,\n","                        subsample=0.8,\n","                        silent=0,\n","                        objective='reg:linear',\n","                        num_round=100)\n","\n","* Training the XGBoost Model\n","\n","xgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"76465e47a370c54d8b8d0510367d6f7bd57411eb"},"outputs":[],"source":["from sagemaker.amazon.amazon_estimator import get_image_uri\n","container = get_image_uri(boto3.Session().region_name, 'xgboost')"]},{"cell_type":"markdown","metadata":{"_uuid":"4adc9dc0d9f92c97f9a658da38e63c80e5753881"},"source":["> Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3, which also specify that the content type is CSV."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1864968074264aa65b384f4f470c8f205bb9da09"},"outputs":[],"source":["s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n","s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"]},{"cell_type":"markdown","metadata":{"_uuid":"05a1a11935223313d5e22e53811101fdf85a2474"},"source":["> Let's begin by delineating the training parameters for the estimator. These encompass:\n","\n","1. Selecting the `xgboost` algorithm container.\n","2. Designating the IAM role.\n","3. Specifying the type and quantity of training instances.\n","4. Establishing the S3 destination for output data.\n","5. Setting the algorithm hyperparameters.\n","\n","Following that, we'll employ the `.fit()` function, detailing:\n","\n","1. The S3 location for output data, considering both the training and validation sets.\n","\n","Take note: We are initiating an m4.xlarge instance, currently priced at 22.2 cents per hour. It will remain active throughout the entirety of our model training."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3907ac643f2eeae424703924c61cd8bf939c7c69"},"outputs":[],"source":["sess = sagemaker.Session()\n","\n","xgb = sagemaker.estimator.Estimator(container,\n","                                    role, \n","                                    train_instance_count=1, \n","                                    train_instance_type='ml.m4.xlarge',\n","                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n","                                    sagemaker_session=sess)\n","\n","# These are the default parameters that came with the Targeting Direct Marketing example\n","xgb.set_hyperparameters(max_depth=5,\n","                        eta=0.2,\n","                        gamma=4,\n","                        min_child_weight=6,\n","                        subsample=0.8,\n","                        silent=0,\n","                        objective='reg:linear',\n","                        num_round=100)\n","\n","xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "]},{"cell_type":"markdown","metadata":{"_uuid":"9f943d056c25acd5e6057eaa5b3a6522aca4a211"},"source":["__A Few Notable Observations__\n","\n","The instance launch typically consumes approximately 2 minutes before it initiates. This period might feel lengthy, leaving one staring at the screen. However, once operational, the algorithm's training process merely requires 39 seconds, coinciding with the duration we are billed. Remarkably efficient, wouldn't you say? Consider the analogy: borrowing someone else's computer for just 39 seconds.\n","\n","Yet, intriguingly, a process spanning 39 seconds translates into nearly 3 minutes of wall clock time."]},{"cell_type":"markdown","metadata":{"_uuid":"6e23c8e724a56cfc7fac9f829daf69ad4d8ffcee"},"source":["### Model Deployment\n","\n","Having successfully trained the `xgboost` algorithm on our dataset, the next step is to deploy a model accessible via a real-time endpoint.\n","\n","This marks the initiation of our third instance. To clarify, the first instance serves as the environment for running this notebook, while the second instance handles the model training. Now, this third instance is dedicated to conducting inferences."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"094e4ed084df6d98d8d9d9a7d88969162ac6b901"},"outputs":[],"source":["xgb_predictor = xgb.deploy(initial_instance_count=1,\n","                           instance_type='ml.m4.xlarge')"]},{"cell_type":"markdown","metadata":{"_uuid":"28da14b4148e42c964218917135298f04a45fd7d"},"source":["__Observations__\n","Again, starting a new instance takes several minutes."]},{"cell_type":"markdown","metadata":{"_uuid":"65a5160075e3df26ecde46c270c2c2fef32896b3"},"source":["### Initial Evaluation of the SageMaker Model\n","\n","Before proceeding, let's assess the performance of our SageMaker (SM) model in predicting the SalePrice values we already possess.\n","\n","To facilitate data transmission to and from our endpoint, we need to establish a method. Presently, our data exists as NumPy arrays within the memory of our notebook instance. To transmit it via an HTTP POST request, we'll serialize it into a CSV string and subsequently decode the resultant CSV.\n","\n","*Note: It's imperative to bear in mind that for inference using the CSV format, SageMaker XGBoost mandates that the data excludes the target variable.*"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4de65a21716823b23eed8598cee37432089de987"},"outputs":[],"source":["xgb_predictor.content_type = 'text/csv'\n","xgb_predictor.serializer = csv_serializer"]},{"cell_type":"markdown","metadata":{"_uuid":"40db0ff71c9161aeb5c69c696f13dcee1aa740de"},"source":["Like we did for our training data, we need to convert the categorical columns in the \"test\" data provided by Kaggle to numerical format."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b41ba70648946858090b4886ed7ec5b33486ba17"},"outputs":[],"source":["df_competition = pd.get_dummies(df_competition)\n","df_competition.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"be0eaa2fa3b2b02bcefa169496d6e72a4024faee"},"outputs":[],"source":["df_train.shape"]},{"cell_type":"markdown","metadata":{"_uuid":"1a5f08b1c96be82fb48738ee72e8d9090457a95f"},"source":["__Note__: Our competition test data differs from our training data by 18 columns. One column's absence is expected, as 'SalePrice' is not included in the Kaggle test data. The remaining disparities stem from variations in values across different columns, especially after categorical data transformation into numerical data via `get_dummies()`.\n","\n","Given that Random Forest, which includes XGBoost algorithms, necessitates a fixed number of columns, I've opted to pad our competition data with the requisite additional columns. I must admit uncertainty regarding the impact of these surplus columns on the XGBoost algorithm. My strategy assumes their negligible influence due to their lack of distribution in the data.\n","\n","A more optimal approach would involve reading and processing all data, both training and competition sets, simultaneously."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"83266c0aae3f67e1477872e8c714ee7a0fba1eed"},"outputs":[],"source":["df_comp_padded=df_competition\n","for x in range(0, 18):\n","    df_comp_padded[x] = pd.Series(1, index=df_comp_padded.index)\n","df_comp_padded.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e9eff318aa66d35378d28c6b231a70c2aeb077a2"},"outputs":[],"source":["df_comp_padded.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8e794af06396c73d3b17c2f3c1dfcb8e1126ea6d"},"outputs":[],"source":["def predict(data):\n","    ids = data['Id']\n","    saleprice = np.array(xgb_predictor.predict(data.as_matrix()).decode('utf-8').split(',')).astype(np.float)\n","    predictions = list(zip(ids,saleprice))\n","    return predictions\n","\n","#predictions = predict(test_data.drop(['SalePrice'], axis=1).as_matrix())\n","#predictions = predict(kaggle_data.drop(['SalePrice'], axis=1).as_matrix())\n","#predictions = predict(kd.as_matrix())\n","%time predictions = predict(df_comp_padded)"]},{"cell_type":"markdown","metadata":{"_uuid":"61be382701208292b78ebc204437e8d725ff5104"},"source":["__Observations__\n","Asking the model to predict the saleprice of 1459 houses took about one half second. That feels pretty quick for real-time human interaction.\n","\n","Let's take a quick look at the predictions:"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bea96d97574c0c9ec710a28ab59210c1bd4b1b6f"},"outputs":[],"source":["predictions[0:5]"]},{"cell_type":"markdown","metadata":{"_uuid":"158761eaf7b9fafb6abc2ca3b40f10102a4cef8a"},"source":["These results deviate from our earlier trivial submission based on means. Let's proceed with submitting them and evaluating the performance of the default XGBoost algorithm.\n","\n","### First SM submission"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8bb392f5accb1f1656ceb0dfc80fd2f74eba4492"},"outputs":[],"source":["np.array(predictions).shape"]},{"cell_type":"markdown","metadata":{"_uuid":"37beeb6e382914af22ef52c80bdbc8bafcbbc04d"},"source":["In terms of shape, it's consistent with the submission requirements, minus the colum headers."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"988c3f4723d13444702aeb85211cd0bc138c0b82"},"outputs":[],"source":["#np.savetxt(\"../data/housing.csv\", predictions, delimiter=\",\", header='Id,SalePrice', fmt='%u')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e979ca2e0e15e2fe6c7907b526108925fcbe06c7"},"outputs":[],"source":["df = pd.DataFrame(predictions, columns=['Id', 'SalePrice'])\n","print(df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5bb96bb59cf92b08142453cb56b2f9b189262a47"},"outputs":[],"source":["df.to_csv('./data/sub_xgboost_default.csv', header=True, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a1263923b8bf4a108492e97035b4b4cd8d09a773"},"outputs":[],"source":["!kaggle competitions submit -f ./data/sub_xgboost_default.csv -m \"Default XGBoost submission\" house-prices-advanced-regression-techniques\n","#\n","# Damned scroll bar"]},{"cell_type":"markdown","metadata":{"_uuid":"de04a8dcac505a72afab5485c76d9106b29637a8"},"source":["This default XGBoost submittion gives me a public score of 0.17203, which is good for 3640th place as of this writing, a significant improvement on our means-based result! We thus have good confidence we've improved our modeling."]},{"cell_type":"markdown","metadata":{"_uuid":"e6d342486fdc06d65ec2d67ae0227a66de2e8053"},"source":["## Hyperparameters \n","\n","### Configuration\n","\n","Now, let's leverage one of SageMaker's strengths: automated hyperparameter tuning!\n","\n",">*Please note, the default settings below indicate that the hyperparameter tuning job might take around 30 minutes to complete.*\n","\n","With our dataset prepared, we're poised to train models. Before diving in, it's crucial to acknowledge the presence of algorithm settings called \"hyperparameters.\" These parameters can significantly influence the performance of trained models. For instance, the XGBoost algorithm encompasses numerous hyperparameters, and selecting the right values for these is paramount to achieving desired training results. However, identifying the optimal hyperparameter setting is complex and often requires systematic exploration.\n","\n","This is where SageMaker's hyperparameter tuning comes into play. We'll automate this exploration process effectively. Specifically, we'll define a range or list of potential values for each hyperparameter we intend to tune. SageMaker hyperparameter tuning will then orchestrate multiple training jobs with varied hyperparameter settings. It evaluates these jobs based on a predefined \"objective metric\" and adapts subsequent attempts based on previous results. Each hyperparameter tuning job operates within a set budget, completing after a specified number of training jobs.\n","\n","Let's configure the hyperparameter tuning job by defining a JSON object that specifies the following information:\n","- The ranges of hyperparameters we aim to tune\n","- The total number of training jobs and the level of parallelism (i.e., how many jobs run simultaneously). While higher parallelism speeds up tuning, it might compromise accuracy. For this example, we'll set a higher parallelism value to keep it concise.\n","- The objective metric used for evaluating training results. In this example, we'll select *validation:auc* to maximize its value throughout the tuning process. Note: the chosen objective metric must be emitted by the algorithm during training.\n","\n","In this instance, we'll tune four hyperparameters:\n","- *eta*: Step size shrinkage used in updates to prevent overfitting.\n","- *alpha*: L1 regularization term on weights.\n","- *min_child_weight*: Minimum sum of instance weight needed in a child.\n","- *max_depth*: Maximum depth of a tree.\n","\n","It's essential to recognize that I'm uncertain whether these are the correct parameters to tune, or if their ranges are appropriate. At this stage, our primary focus is on ensuring the end-to-end process functions smoothly. We have our baseline result to safeguard against any deterioration."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b7c84380bfa46c06fcdbc2369eb43f70a41a8979"},"outputs":[],"source":["from time import gmtime, strftime, sleep\n","tuning_job_name = 'xgboost-tuningjob-' + strftime(\"%d-%H-%M-%S\", gmtime())\n","\n","print (tuning_job_name)\n","\n","tuning_job_config = {\n","    \"ParameterRanges\": {\n","      \"CategoricalParameterRanges\": [],\n","      \"ContinuousParameterRanges\": [\n","        {\n","          \"MaxValue\": \"1\",\n","          \"MinValue\": \"0\",\n","          \"Name\": \"eta\",\n","        },\n","        {\n","          \"MaxValue\": \"10\",\n","          \"MinValue\": \"1\",\n","          \"Name\": \"min_child_weight\",\n","        },\n","        {\n","          \"MaxValue\": \"2\",\n","          \"MinValue\": \"0\",\n","          \"Name\": \"alpha\",            \n","        }\n","      ],\n","      \"IntegerParameterRanges\": [\n","        {\n","          \"MaxValue\": \"10\",\n","          \"MinValue\": \"1\",\n","          \"Name\": \"max_depth\",\n","        }\n","      ]\n","    },\n","    \"ResourceLimits\": {\n","      \"MaxNumberOfTrainingJobs\": 20,\n","      \"MaxParallelTrainingJobs\": 3\n","    },\n","    \"Strategy\": \"Bayesian\",\n","    \"HyperParameterTuningJobObjective\": {\n","      \"MetricName\": \"validation:rmse\",\n","      \"Type\": \"Minimize\"\n","    }\n","  }"]},{"cell_type":"markdown","metadata":{"_uuid":"286646c4172e38af20014c24177729c3716d43e3"},"source":["Next, we'll configure the training jobs that the hyperparameter tuning job will initiate. This involves defining a JSON object specifying the following details:\n","\n","- The container image for the algorithm (XGBoost).\n","- Input configuration for the training and validation data.\n","- Configuration for the output of the algorithm.\n","- The values of any algorithm hyperparameters not tuned in the tuning job (StaticHyperparameters).\n","- The type and quantity of instances for the training jobs.\n","- The stopping condition for the training jobs.\n","\n","Again, as we're utilizing the built-in XGBoost algorithm, it emits two predefined metrics: *validation:auc* and *train:auc*. However, for our Kaggle competition, we require \"reg:linear\", or considering my uncertainty, something closer to \"reg:linear\". \n","\n","It's important to note that if you bring your own algorithm, your algorithm will emit metrics independently. In such cases, you'll need to incorporate a MetricDefinition object to define the format of those metrics using regex, enabling SageMaker to extract them accurately."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c1f16bb368960e6989e7b2ec1c9eb6c2540247c6"},"outputs":[],"source":["from sagemaker.amazon.amazon_estimator import get_image_uri\n","training_image = get_image_uri(region, 'xgboost', repo_version='latest')\n","     \n","s3_input_train = 's3://{}/{}/train'.format(bucket, prefix)\n","s3_input_validation ='s3://{}/{}/validation/'.format(bucket, prefix)\n","    \n","training_job_definition = {\n","    \"AlgorithmSpecification\": {\n","      \"TrainingImage\": training_image,\n","      \"TrainingInputMode\": \"File\"\n","    },\n","    \"InputDataConfig\": [\n","      {\n","        \"ChannelName\": \"train\",\n","        \"CompressionType\": \"None\",\n","        \"ContentType\": \"csv\",\n","        \"DataSource\": {\n","          \"S3DataSource\": {\n","            \"S3DataDistributionType\": \"FullyReplicated\",\n","            \"S3DataType\": \"S3Prefix\",\n","            \"S3Uri\": s3_input_train\n","          }\n","        }\n","      },\n","      {\n","        \"ChannelName\": \"validation\",\n","        \"CompressionType\": \"None\",\n","        \"ContentType\": \"csv\",\n","        \"DataSource\": {\n","          \"S3DataSource\": {\n","            \"S3DataDistributionType\": \"FullyReplicated\",\n","            \"S3DataType\": \"S3Prefix\",\n","            \"S3Uri\": s3_input_validation\n","          }\n","        }\n","      }\n","    ],\n","    \"OutputDataConfig\": {\n","      \"S3OutputPath\": \"s3://{}/{}/output\".format(bucket,prefix)\n","    },\n","    \"ResourceConfig\": {\n","      \"InstanceCount\": 1,\n","      \"InstanceType\": \"ml.m4.xlarge\",\n","      \"VolumeSizeInGB\": 10\n","    },\n","    \"RoleArn\": role,\n","    \"StaticHyperParameters\": {\n","      \"eval_metric\": \"rmse\",\n","      \"num_round\": \"100\",\n","      \"objective\": \"reg:linear\",\n","      \"rate_drop\": \"0.3\",\n","      \"tweedie_variance_power\": \"1.4\"\n","    },\n","    \"StoppingCondition\": {\n","      \"MaxRuntimeInSeconds\": 43200\n","    }\n","}"]},{"cell_type":"markdown","metadata":{"_uuid":"ce28e0ee50e812c5b9b4c00e3859c8c13fc10cbd"},"source":["__Launch Hyperparameter Tuning__\n","\n","To commence the hyperparameter tuning process, we'll invoke the `create_hyper_parameter_tuning_job` API. Once initiated, we can monitor the progress of the hyperparameter tuning job through the SageMaker console until its completion.\n","\n","__Costs__\n","\n","Considering we've specified three parallel jobs, three instances will be engaged. Consequently, we'll incur three times the cost per hour compared to our original training."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"af2b326b25c5a86463b9300eefb6f79c3e7a80a9"},"outputs":[],"source":["smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n","                                            HyperParameterTuningJobConfig = tuning_job_config,\n","                                            TrainingJobDefinition = training_job_definition)"]},{"cell_type":"markdown","metadata":{"_uuid":"fe83e6c35319e14c4d0411218411e9db9e7c1b6f"},"source":["Returning to the SageMaker dashboard, we'll find our three concurrent training jobs in progress!\n","\n","Before proceeding further, let's perform a quick check of the hyperparameter tuning job's status to ensure it commenced successfully."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"fe0cbf28cef2b5a53ccf4d84de0ad41760f77f1d"},"outputs":[],"source":["smclient.describe_hyper_parameter_tuning_job(\n","    HyperParameterTuningJobName=tuning_job_name)['HyperParameterTuningJobStatus']"]},{"cell_type":"markdown","metadata":{"_uuid":"a25cbba4f1f10e9278fc87287993876a97832f50"},"source":["### Analyzing Tuning Job Results\n","\n","To delve into the outcomes of the hyperparameter tuning process, please consult \"HPO_Analyze_TuningJob_Results.ipynb\" for a glimpse into example code for analyzing the tuning job results.\n","\n","Despite my best efforts, the tuning job returned different parameters for XGBoost. Surprisingly, implementing these adjustments yielded nearly identical results on the public leaderboard. Consequently, I've opted not to burden you with redundant code for retraining the algorithm and resubmitting to the competition.\n","\n","This discrepancy could stem from various factors. It's plausible that I selected the wrong parameters for fine-tuning. Alternatively, the most substantial improvements might lie elsewhere, such as in feature engineering, outlier removal, or the inadvertent influence of spurious columns.\n","\n","Nonetheless, we now possess a pipeline that facilitates further experimentation and exploration!"]},{"cell_type":"markdown","metadata":{"_uuid":"33aeb760cf7b08218d2827df0863831e45cbb879"},"source":["### Tidying Up\n","\n","Once you've completed your tasks with this notebook, execute the cell below. This action will eliminate the hosted endpoint you created, ensuring there are no charges incurred from an unattended instance."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"29299d5b6bf6c02900b3f46a0b3ac21f99c95f44"},"outputs":[],"source":["sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"]},{"cell_type":"markdown","metadata":{"_uuid":"121406904a4e4c48bf35b1f52b3f6ab66af92572"},"source":["## Final Thoughts\n","\n","Reflecting on my experience with SageMaker, I find it to be largely positive. The platform's intuitive interface coupled with a wealth of pre-built examples makes it accessible and adaptable to various use cases. Despite utilizing automated hyperparameter tuning, I'm cognizant that I've only scratched the surface of SageMaker's potential.\n","\n","However, a notable drawback I encountered revolves around the time required for instances to initialize. This delay, particularly during casual data exploration, often disrupted my workflow and impeded real-time interaction. While enhancing my proficiency in instance management could alleviate this issue, exploring alternatives such as Crestle or Papermaker might offer smoother data exploration and experimentation experiences.\n","\n","To sum up, SageMaker proves to be an excellent tool for \"real\" research and development or production scenarios, where its robust features are indispensable. However, for casual data exploration and machine learning studies, turn-key solutions may provide a more efficient experience."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":19081,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"notice":"Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."},"nbformat":4,"nbformat_minor":4}
